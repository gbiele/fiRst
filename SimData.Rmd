---
title: "SimulatingData"
author: "Guido Biele"
date: "6/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this document we will briefly look at how to simulate data and R, which we will use to estimate the statistical power (1-\beta) for linear mixed models. In particular, we will build up complexity by going through following steps:

* Simulating univariate data in R
* Simulating multivariate data in R
* Estimating power in linear regression
* Estimating power in a hierarchical linear regression

## Simulating univariate data in R

A basic way to simulate data in R is to simulate data from a distributions. For example 

```{r simulate_normal}
rdata = rnorm(50, mean = 100, sd = 15)
```

generates 100 random numbers from a normal distribution with a mean of 100 and a standard deviation of 10. We can plot and verify this:

```{r show_sim_results}
hist(rdata)
mean(rdata)
sd(rdata)
```

### A referesher about loops in R.
Lets assume you have a vector of elements `Ks`and want to add one to each element. This can be done by
```{r loop_refresher1, eval=F}
for (k in Ks) {
  k + 1
}
```
Where the different parts have the meaning

* `for (k in Ks)` means for each element in Ks, which has the name k inside the loop
* `{}` do what is in the curly brackets

It is important to know that the operations in `{}`can access and change variables outside the loop. Therefore, in the following loop

```{r loop_refresher2, eval = F}
age = 1
years = (2000:2015)
for (y in years) {
  age = age + 1
}
```

age will have a new value after the loop is completed. What is this value?


As we can see, the mean is not exactly 100 and the standard deviation is not exactly 15. This is expected with such a small sample size. However, if we redo this many times, we see that the mean of our means is 0:

```{r se_of_mean}
K = 10000
means = vector(length = K)
sds = vector(length = K)
for (k in 1:K) {
  rd = rnorm(50,mean = 100, sd = 15)
  sds[k] = sd(rd)
  means[k] = mean(rd)
}
mean(means)
mean(sds)
```

And the standard error of our estimates are:

```{r show_ses}
sd(means)
sd(sds)
```

Note that the standard deviation of our means is simply $\frac{sd}{\sqrt{N}}$ '15/sqrt(50)' ([there is not such formula for the standard error of the standard deviation](https://stats.stackexchange.com/questions/156518/what-is-the-standard-error-of-the-sample-standard-deviation)). 

The key thing to remember here is that we can generate random data from a distribution in r by using a function that has the form r<b>_dist_</b> , where <b>_dist_</b> is the name or abbreviation for a distribution. For example

* `rnorm` generates normally distributed data
* `rbinom` generates data from a binomial distribution 
* `rpoiss` generates data from a Poisson distribution

[Here](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Distributions.html) is a list of distributions that can be found in base R. [Additional packages](https://cran.r-project.org/web/views/Distributions.html) provide more distributions.

The basic distributions we have looked at so far are univariate, because they to not allow us to specify relationships between multiple variables from that distributions. To see this, we can simply plot to vectors of normally distributed data and add a regression line:

```{r uncorrelated_variables}
v1 = rnorm(500)
v2 = rnorm(500)
plot(v1,v2)
abline(lm(v2~v1))
```

## Simulating multivariate data in R

### A vary basic example for a data generative model

In we want to simulate dependent data, one way is to say explicitly what the data generating process is and simulate the data accordingly. For example, if we assume that variable 'x' explains 50% of the variance of y, we can simply write:

```{r simulate_from_DGP}
x = rnorm(1000)
e = rnorm(1000)
y = x+e
```

This will produce variables x and y, which are correlated with r = .707:

```{r plot_simulation_from_DGP}
plot(x,y,
     main = paste("r=",cor(x,y)))
```

How do we know that the correlation should be .707? Because in our simple model `x` explains 50% of the variance of `y`. (`e` and `y` have the same standard deviation, and their weights are equal, which is easier to see if we write `y = 1*y + 1*e`.) The relationship between correlation and explained variance is $r = \sqrt{explained\ variance}$, so that can say $r = sqrt(.5)$, or in R  `sqrt(.5)` =  `r sqrt(.5)`.

### Simulating correlated responses to licker scales.

However, we are typically thinking in terms of correlations or covariances, and not necessarily in terms of explain variance, and we sometimes have multiple variables for which we want to describe their relationship. In R, we can use the `rmvnorm` function from the `mvtnorm` package to generate multivariate data. Let us for example say that we want to simulate 5 items from a Likert scale with 5 response categories, such that that between item correlations are $r \sim 0.7$. To do this, we 

* first generate correlated normally distributed variables
* and than transform these normally distributed variables into ordinal responses.

Lets start with generating 5 normally distributed variables. Type `?rmvnorm` in the Console to see how to use the functions. We need

* `n`, the number of observations
* `mean` a vector with means for the variables
* `sigma` the covariance matrix.

The mean vector is easy:

```{r likert_mean}
k = 5
m = rep(0,k)
```

Covariance matrix is a symmetric matrix, where values in the main diagonal are the variances and those in the off diagonal are the co-variances. For example:

```{r likert_cov1}
sigma = matrix(NA,ncol = k, nrow = k)
sigma
```

generates a matrix with only NAs. Now we can set all variances to be 1 by:

```{r likert_cov2}
diag(sigma) = 1
sigma
```

If all variances are 1, the covariance matrix is also a correlation matrix. Next we set all covariances to be around .7. Note that with k variables, we have `(k^2-k)/2` = 10 covariances:

```{r likert_cov3}
sigma[lower.tri(sigma)] = .7 + (runif(10)-.5)/20
sigma
```

Now that we have filled the lower triangle, we need to fill the upper triangle so that everything is symmetric (`sigma[2,3] = sigma[3,2])).

```{r likert_cov4}
sigma[upper.tri(sigma)] = t(sigma)[upper.tri(sigma)]
sigma
```

This was already the hardest part, because generating the multivariate normal distributions is now simply:

```{r likert_sim_latent}
library(mvtnorm)
mvdata = rmvnorm(500,mean = m, sigma = sigma)
cor(mvdata)
round(cor(mvdata)-sigma,digits = 2)
```

The last step is to get ordered categorical data from this. Item response models simply assume that there is an underlying latent ability, and cut points that split the continuous ability into discrete levels. Lets assume the all items have the same difficulty level and discriminate at the same ability levels, i.e. lets give all items the same cut points:

```{r likert_cut}
cut_points = c(-Inf,1,1.5,2,Inf)

likert_items = data.frame(mvdata)
for (v in 1:k)
  likert_items[,v] = cut(likert_items[,v],
                         breaks = cut_points,
                         ordered_result = T)

```

Any idea how this data might look like? Lets see:

```{r likert_plot}
barplot(table(likert_items[,1]))
```

So this looks a lot like items from clinical scales, which do not discriminate in the bulk of the distribution (all values below 1 here). What simulation can teach us here is that if we use the proper way to describe the covariation between these variables (polychoric correlation) we can more or less recover it correctly. However, if we just treat it as a simple numeric variable, we underestimate the correlation between variables:

```{r likert_correlation}
library(polycor)
nc = pc = matrix(NA,ncol = k,nrow = k)
for (j in 1:(k-1)) {
  for (i in (j+1):k) {
    pc[i,j] = pc[j,i] = polychor(likert_items[,i],
                                 likert_items[,j])
    nc[i,j] = nc[j,i] = cor(as.numeric(likert_items[,i]),
                            as.numeric(likert_items[,j]))
  }
}
pcs = pc[lower.tri(sigma)]
ncs = nc[lower.tri(sigma)]
truth = sigma[lower.tri(sigma)]

plot(truth,pcs, ylim = range(c(ncs,pcs)))
points(truth,ncs, col = "red", pch = "x")
abline(0,1, col = "green4", lwd = 2)
abline(lm(ncs~truth), lty = 2, col = "red")
abline(lm(pcs~truth), lty = 2)
```

The green line is the identity line, which shows us that using simple Pearson correlation severely underestimates the true correlations.

## Calculating power by simulation

Statistical power is related to errors in statistical inference. Here the Type I error is a failure to reject the H0 when it is true (\alpha). The Type II error is the failure to reject the H0 when it is in fact false (\beta). Statistical power is then defined as 1-\beta, and colloquially names as the probability to detect an effect if there is one. I say "colloquially" here, because this statement is strictly speaking incorrect, as null hypothesis testing does not allow to make statements about the presence of an effect (NHST only allows statements about the probability of the observed data, given the H0 is true). Before we finally do some simulations, I'll add that while we use statistical power as an example to learn about simulating data in R, I'd not do NHST but Bayesian statistics.

Anyhow, with this out of the way, here is the general procedure to estimate statistical power of a regression analysis by simulation:

1. we determine important variables, in particular the sample size, the covariation of all explanatory variables,and the regression weights. In the latter, the regression coefficient for our exposure, which must not be 0, is the effect size for which we want to obtain a power estimates.
2. We repeat following steps many times
    + simulate data given the variables defined in step 1
    + check if the regression coefficient for our exposure variables is significant 
3. We calculate power as the proportion of tests in step 2. that were significant.

OK, lets fix some variables:

```{r lm_sim_vars}
N = 5000
number_covariates = 3
effect_size = .3
effect_size_covariates = .1
beta = c(effect_size,
         rep(effect_size_covariates,
             number_covariates))

cov_exposure_covariates = .01
cov_covariates = .15

k = 1 + number_covariates
```

With this information, we can build our covariance matrix:
```{r lm_sim_sigma}
sigma = matrix(NA,
               nrow = k,
               ncol = k)

diag(sigma) = 1
sigma[1,2:k] = sigma[2:k,1] = cov_exposure_covariates
sigma[is.na(sigma)] = cov_covariates
```

Which we then can use to generate our predictors X, 
```{r lm_sim_simulate}
X = rmvnorm(N,
            mean = rep(0,k),
            sigma = sigma)
y =  X %*% beta + rnorm(N)
dta = data.frame(cbind(y,X))
names(dta) = c("y","x", paste0("cov",1:3))
```

To check that we crated data as intended, we can run a linear regression and look at the coefficients, which should be identical with the variables `effect_size` and `cov_exposure_outcomes`:

```{r lm_sim_check}
rbind(
coef(lm(y~x + cov1 + cov2 + cov3, dta))[-1],
beta)
```

This looks good. To repeatedly do this, we can package these steps into functions.

### A brief description of functions in R
Functions in R take the form

```{r functions_intro1, eval = F}
function_name = function(parameter1 = 1, parameter2 = NULL) {
  ## do something with the provided parameters
  return(something_generate_in_the_function)
}
```

Lets make this clearer with a simple example:
```{r functions_intro2}
devide = function(dividend,divisor) {
  quotient = dividend/divisor
  return(quotient)
}
```

Functions can access variables defined outside the function, but they wont change them. For example
```{r functions_intro3}
decimals = 2
devide = function(dividend,divisor) {
  quotient = dividend/divisor
  quotient = round(quotient,decimals)
  return(quotient)
}
devide(5,7)
```

works and will use the decimal number defined before the function. However, this function will not work if we delete the decimal variable:
```{r functions_intro4, error=TRUE}
rm(decimals)
devide(5,7)
```

If we add decimals to the function parameters, this value takes precedence over the value defined outside the function:

```{r functions_intro5}
decimals = 2
devide = function(dividend,divisor, decimals = 1) {
  quotient = dividend/divisor
  quotient = round(quotient,decimals)
  return(quotient)
}
devide(5,7)
```

This example also shows us that we can provide default values that will be used, and overridden if we provide our own value:

```{r functions_intro6}
devide(5,7, decimals = 3)
```

Note that one could also write:

```{r functions_intro7}
devide(5,7,4)
devide(decimals = 4, divisor = 7, dividend = 5)
```

What we see here is that if we provide parameter values without names to a function, it is assumed that we submit parameters in the order in which they are defined in the function signature. If we want to submit them in a different order, or keep only some default values, we need to submit the parameters for which we want to specify the value together with their name.


A function to generate data, given parameters:

```{r lm_sim_f_make_lm_data}
make_lm_data = function(effect_size,
                        N = 100,
                        effect_size_covariates = 0,
                        number_covariates = 3,
                        cov_exposure_covariates = 0,
                        cov_covariates = .2) {
  beta = c(effect_size,
           rep(effect_size_covariates,number_covariates))
  
  k = 1 + number_covariates
  sigma = matrix(NA,
                 nrow = k,
                 ncol = k)
  diag(sigma) = 1
  sigma[1,2:k] = sigma[2:k,1] = cov_exposure_covariates
  sigma[is.na(sigma)] = cov_covariates
  
  X = rmvnorm(N,
              mean = rep(0,k),
              sigma = sigma)
  
  y =  X %*% beta + rnorm(N)
  dta = data.frame(cbind(y,X))
  names(dta) = c("y","x", paste0("cov",1:number_covariates))
  return(dta)
}
```

The names for the parameters of the function are very long here, because this makes understanding easier. The drawback is that the function looks more complicated. Typically, people use shorter names for parameters, which makes the function look more transparent. To be still understandable, the function then requires some documentation

```{r eval = F}
make_lm_data = function(es, N = 100, es.cv = 0, n.cv = 3, cov.ecv = 0, cov.cv = .2) {
#' Simulate data for a linear regression, given following parameters
#' 
#' @param es effect size of expsoure.
#' @param N sample size.
#' @param es.cv effect size of covariates (adjustment variables).
#' @param n.cv number of covariates.
#' @param cov.ecv covariance between exposure and covariates.
#' @param cov.cv covariance among covariates.
#' @return data.frame with outcome y, exposure x and covariates cv...
#' @examples
#' make_lm_data(.2)
#' make_lm_data(.2, N = 500, es.cv .2, n.cv = 3, cov.ecv = .5, cov.cv = .2)
  
  beta = c(es, rep(es.cv,n.cv))
  
  k = 1 + n.cv
  sigma = matrix(NA, nrow = k, ncol = k)
  diag(sigma) = 1
  sigma[1,2:k] = sigma[2:k,1] = cov.ecv
  sigma[is.na(sigma)] = cov.cv
  
  X = rmvnorm(N, mean = rep(0,k), sigma = sigma)
  
  y =  X %*% beta + rnorm(N)
  dta = data.frame(cbind(y,X))
  names(dta) = c("y","x", paste0("cov",1:n.cv))
  return(dta)
}
```

Back to our function for estimating power by simulation:
A function to fit a linear regression and extract the p-value for the exposure x.
```{r lm_sim_f_get_pval}
get_pval = function(dta,f) {
  lmfit = lm(f,dta)
  return(summary(lmfit)$coefficients["x",4])
}
```

And a function that uses the first to function to repeatedly generate data, fit the linear regression, and extract the relevant p-value.
```{r lm_sim_f_sim_power}
sim_power = function(effect_size,
                     N = 100,
                     k = 500,
                     effect_size_covariates = .1,
                     number_covariates = 3,
                     cov_exposure_covariates = 0,
                     print.power = T) {
  
  f = as.formula(paste("y ~ x + ",
                       paste0("cov",1:number_covariates, collapse = " + ")))
  
  pvals = vector(length = k)
  for (i in 1:k) {
    dta = make_lm_data(effect_size,
                     N,
                     effect_size_covariates = effect_size_covariates,
                     number_covariates = number_covariates,
                     cov_exposure_covariates = cov_exposure_covariates)
    pvals[i] = get_pval(dta,f)
  }
  if (print.power == T) 
    print(paste("Power =", mean(pvals<.05)))
  return(pvals)
}
```

Now we can estimate the power for different parameter configurations. Lets start with an effect size of 0.1 and N = 100:

```{r lm_sim_simulate1}
pvals = sim_power(effect_size = .1,
                  N = 100)
```

We can increase sample size or effect size:

```{r lm_sim_simulate2}
pvals = sim_power(effect_size = .2,
                  N = 100)
pvals = sim_power(effect_size = .1,
                  N = 500)
```


And we can plot how power changes when we fix one and let the other vary:

```{r lm_sim_simulate3}
Ns = c(50, 100, 250, 500, 1000, 2500)
ess = c(.05, .075, .1, .15, .2)

power_stats = c()
for (N in Ns) {
  for (es in ess) {
    pvals = sim_power(effect_size = es, N = N, print.power = F)
    power = mean(pvals < .05)
    power_stats = rbind(power_stats,
                        c(N, es, power))
  }
}
power_stats = data.frame(power_stats)
names(power_stats) = c("N","es","power")
```

Lets plot this: 

```{r plot_lm_sim_simulate3}
library(ggplot2)
ggplot(power_stats, aes(x = N, y = power, color = factor(es))) + 
  geom_line() +
  geom_hline(yintercept = .8)
```

This analysis assumed that the three covariates were independent of the exposure and had a small effect on the outcome. Lets change that and assume a small correlation (r = .1) between exposure and covariates.

```{r lm_sim_simulate4}
power_stats_a = power_stats
power_stats_a$covec = "0"

power_stats = c()
for (N in Ns) {
  for (es in ess) {
    pvals = sim_power(effect_size = es,
                      N = N,
                      cov_exposure_covariates = .3,
                      print.power = F)
    power = mean(pvals < .05)
    power_stats = rbind(power_stats,
                        c(N, es, power))
  }
}
power_stats = data.frame(power_stats)
names(power_stats) = c("N","es","power")
power_stats_b = power_stats
power_stats_b$covec = ".3"

power_stats = rbind(power_stats_a,
                    power_stats_b)

ggplot(power_stats, aes(x = N, y = power, color = factor(es), lty = covec)) + 
  geom_line() +
  geom_hline(yintercept = .8)
```

And we see (as expected) that covariation between exposure and adjustment variable reduces power.
